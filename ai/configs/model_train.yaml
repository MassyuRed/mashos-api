base_model: "mistral-7b"
method: "lora"
dtype: "bfloat16"
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
max_seq_len: 2048
micro_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 1.2e-4
num_epochs: 3
warmup_ratio: 0.03
weight_decay: 0.0
save_steps: 500
eval_steps: 500
output_dir: "models/lora"
train_file: "data/train/{{AI_NAME}}_interpret_train.jsonl"
